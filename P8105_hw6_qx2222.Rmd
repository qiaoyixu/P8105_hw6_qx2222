---
title: "P8105_hw6_qx2222"
author: "Qiaoyi Xu"
date: "2022-12-01"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


```{r}
library(tidyverse)
library(modelr)
library(mgcv)
```

## Problem 1 (answer posted)

To obtain a distribution for $\hat{r}^2$, we'll follow basically the same procedure we used for regression coefficients: draw bootstrap samples; the a model to each; extract the value I'm concerned with; and summarize. Here, we'll use `modelr::bootstrap` to draw the samples and `broom::glance` to produce `r.squared` values. 

```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```


```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1 may be a cause for the generally skewed shape of the distribution. If we wanted to construct a confidence interval for $R^2$, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. However, because the shape isn't symmetric, using the mean +/- 1.96 times the standard error probably wouldn't work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a similar approach, with a bit more wrangling before we make our plot.

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

As with $r^2$, this distribution is somewhat skewed and has some outliers. 

The point of this is not to say you should always use the bootstrap -- it's possible to establish "large sample" distributions for strange parameters / values / summaries in a lot of cases, and those are great to have. But it is helpful to know that there's a way to do inference even in tough cases. 


## Problem 2

### Data import
```{r}
homicide = read_csv("data/homicide-data.csv") #import 'homicide' data
```

### Data cleaning
```{r}
homicide = homicide %>%
  mutate(city_state = str_c(city, ", ", state)) %>%
  mutate(homicide_status = case_when(
         disposition == "Closed without arrest" ~ 0,
         disposition == "Open/No arrest" ~ 0,
         disposition == "Closed by arrest" ~ 1))%>%
  filter(!(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"))) %>% #omit observations
  filter(victim_race %in% c("Black", "White")) %>% #limit your analysis those for whom victim_race is white or black
  mutate(victim_age = as.numeric(victim_age)) %>%  # Be sure that victim_age is numeric
  drop_na(victim_age, victim_sex,victim_race, homicide_status)

homicide
```

### For the city of Baltimore, MD
```{r, warning=FALSE}
fit_log_MD = homicide %>% #use the glm function to fit a logistic regression
  filter(city_state == "Baltimore, MD") %>%
  glm(homicide_status ~ victim_age + victim_sex + victim_race, 
      data = ., family = binomial())%>%
  broom::tidy() %>%
  mutate(OR = exp(estimate),
         CI_lower = exp(estimate - 1.96*std.error),
         CI_higher = exp(estimate + 1.96*std.error)) %>%
  select(term, estimate, OR, CI_lower, CI_higher) %>%
  filter(term == "victim_sexMale") %>%
  knitr::kable(digits = 3, col.names = c("Term", "Estimate", "Odds ratio", "95CI_low","95CI_high"))

fit_log_MD
```

From the output, for the city of Baltimore, MD, the estimated adjusted odds ratio for solving homicides comparing male victims to female victims is 0.426, keeping all other variables fixed. And, the confidence interval is between 0.325 and 0.558.

### Run glm for each of the cities
```{r}
homicide_cities = homicide %>% #use the glm function to fit a logistic regression
  nest(data = -city_state) %>%
  mutate(
    models = map(.x = data, ~ glm(homicide_status ~ victim_age + victim_sex + victim_race, data = .x, family = binomial())),
    output = map (models, broom::tidy))%>%
  select(city_state, output) %>%
  unnest(output) %>%
  mutate(OR = exp(estimate),
         CI_lower = exp(estimate - 1.96*std.error),
         CI_higher = exp(estimate + 1.96*std.error)) %>%
  select(city_state, term, estimate, OR, CI_lower, CI_higher) %>%
  filter(term == "victim_sexMale")

homicide_cities %>%
  knitr::kable(digits = 3, col.names = c("City, state", "Term", "Estimate", "Odds ratio", "95CI_low","95CI_high"))
```

### Create a plot that shows the estimated ORs and CIs for each city
```{r}
prob2_plot = homicide_cities %>%
  mutate(city_state = reorder(city_state, OR)) %>%
  ggplot(aes(x = city_state, y = OR))+
  geom_point(alpha = .5) +
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_higher))+
  labs(x = "City, state", y = "Odds Ratio",
       title = "The estimated ORs and CIs for each city") +
  theme(axis.text.x = element_text(angle = 90))

prob2_plot
```

From this plot, we could see that compared with other cities, the estimated OR for new york city is the lowest and the estimated OR for Albuquerque is the highest for solving homicides comparing male victims to female victims, keeping all variables fixed. Besides, for most cities, the odds of solved status homicides for male victims are lower than the odds of sloved status homicides for female victims.

```{r}
ggsave(
  filename = "results/plot for each city(problem 2).pdf",
  plot = prob2_plot,
  width = 30,
  height = 20,
  units = "cm"
  ) #export plot to 'results' directory
```




## Problem 3

### Load and clean the data 
```{r}
birthweight = read_csv("data/birthweight.csv") #import 'birthweight' data

birthweight
```

```{r}
birthweight = birthweight %>%
  mutate(babysex = as.factor(babysex),
         babysex = recode_factor(babysex, "1" = "Male", "2" = "Female"),
         frace = as.factor(frace),
         frace = recode_factor(frace, 
                               "1" = "White", "2" = "Black", "3" = "Asian", 
                               "4" = "Puerto Rican", "8" = "Other", "9" = "Unknown"),
         malform = as.factor(malform),
         malform = recode_factor(malform, "0" = "Absent", "1" = "Present"),
         mrace = as.factor(mrace),
         mrace = recode_factor(mrace, 
                               "1" = "White", "2" = "Black", "3" = "Asian", "4" = "Puerto Rican", "8" = "Other"))

skimr::skim(birthweight) #check missing data

```

### Propose a regression model for birthweight

As we know, Genetics, age of the parent, length of Pregnancy, mother's habits, baby's gender, ethnicity are important factors to influence baby's birth weight. Besides, I would like to only focus on the mother's condition as factors in the influence of birthweight. So, I took them as predictors into my regression model, such as delwt, gaweeks, mheight, momage, mrace, smoken.

```{r}
birthweight_mymodel = lm(bwt ~ delwt + gaweeks + mheight + momage + mrace + smoken, data = birthweight)
  
birthweight_mymodel %>%
  broom::tidy() %>%
  knitr::kable(digits = 3)

```

### Create a plot of model residuals against fitted values 
```{r}
plot_residual_mymodel = birthweight %>%
  add_predictions(birthweight_mymodel)%>%
  add_residuals(birthweight_mymodel) %>%
  ggplot(aes(x = pred, y = resid, color = resid))+
  geom_point() +
  geom_smooth(se = F, color = "red", method = "lm")+
  labs(x = "Fitted values", y = "Residuals",
       title = "Residuals v.s. Fitted values") 

plot_residual_mymodel
  
```

```{r}
ggsave(
  filename = "results/residual v.s. Fitted Values of mymodel (problem 3).pdf",
  plot = plot_residual_mymodel,
  width = 30,
  height = 20,
  units = "cm"
  ) #export plot to 'results' directory
```

### Compare your model to two others
One using length at birth and gestational age as predictors (main effects only)
One using head circumference, length, sex, and all interactions (including the three-way interaction) between these
```{r}
cv_df = crossv_mc(birthweight,100) %>%
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = 
  cv_df %>% 
  mutate(
    my_mod  = map(train, ~lm(bwt ~ delwt + gaweeks + mheight + momage + mrace + smoken, data = .x)),
    main_effect_mod  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    interaction_mod  = map(train, ~lm(bwt ~ bhead + blength + babysex + 
                                        bhead*blength + bhead*babysex + blength*babysex + bhead*babysex +
                                        bhead*blength*babysex, data = .x))) %>%
  mutate(
    rmse_my_mod = map2_dbl(my_mod, test, ~rmse(model = .x, data = .y)),
    rmse_main_effect_mod = map2_dbl(main_effect_mod, test, ~rmse(model = .x, data = .y)),
    rmse_interaction_mod = map2_dbl(interaction_mod, test, ~rmse(model = .x, data = .y)))

```

```{r}
cv_plot = cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>%
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin()

cv_plot

```

From the 'cv_plot' graph, we could see that the rmse value of my hypothesized model is biggest and the rmse value of the interaction model is lowest. The lower rmse value is, the better the model is. Therefore, the model using head circumference, length, sex, and all interactions (including the three-way interaction) between these is a clear winner!

```{r}
ggsave(
  filename = "results/cross-validated plot (problem 3).pdf",
  plot = cv_plot,
  width = 30,
  height = 20,
  units = "cm"
  ) #export plot to 'results' directory
```






